{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_data(url : str)-> list:\n",
    "    \"\"\" get the json data from provided url and return an array of the data\"\"\"\n",
    "\n",
    "    data = requests.get(url)\n",
    "    raw_data = data.text\n",
    "    data_array = raw_data.split(\"\\n\")\n",
    "    return data_array\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process each element of the list and assign a label of helpful or not based on the score\n",
    "import json\n",
    "from typing import Union\n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "def get_helpful_label(helpful_score: Union[int, float]) -> Union[int,float]:\n",
    "    \"\"\" return label of how helpful \"\"\"\n",
    "\n",
    "    if helpful_score < 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def data_to_df(data: list) -> pd.DataFrame:\n",
    "    \"\"\" convert the data array into a pandas dataframe \"\"\"\n",
    "\n",
    "    df_dict = {}\n",
    "    df_dict[\"text\"] = []\n",
    "    df_dict[\"label\"] = []\n",
    "\n",
    "    for item in data:\n",
    "        try:\n",
    "            item_json = json.loads(item)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        df_dict[\"text\"].append(item_json[\"sentence\"])\n",
    "\n",
    "        # get helpful label from score \n",
    "        helpful_score = item_json[\"helpful\"]\n",
    "        helpful_label = get_helpful_label(helpful_score)\n",
    "        df_dict[\"label\"].append(helpful_label)\n",
    "\n",
    "    # creating dataframe \n",
    "    df = pd.DataFrame.from_dict(df_dict)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "train_data = \"https://helpful-sentences-from-reviews.s3.amazonaws.com/train.json\"\n",
    "test_data = \"https://helpful-sentences-from-reviews.s3.amazonaws.com/test.json\"\n",
    "\n",
    "train_data = get_data(train_data)\n",
    "test_data = get_data(test_data)\n",
    "\n",
    "train_df = data_to_df(train_data)\n",
    "test_df = data_to_df(test_data)\n",
    "\n",
    "train_df.to_csv(\"helpful_sentences_train.csv\", index=False)\n",
    "test_df.to_csv(\"helpful_sentences_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this flash is a superb value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The pictures were not sharp at all.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A very good resource for parents.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We have it in a child's room, and will be swit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Again the makers are too lazy to bring in the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                      this flash is a superb value.      1\n",
       "1                The pictures were not sharp at all.      1\n",
       "2                  A very good resource for parents.      1\n",
       "3  We have it in a child's room, and will be swit...      0\n",
       "4  Again the makers are too lazy to bring in the ...      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "train_data = pd.read_csv(\"helpful_sentences_train.csv\")\n",
    "test_data = pd.read_csv(\"helpful_sentences_test.csv\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model\n",
    "Using HuggingFace DistilBert base model to classify the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.16s/ba]\n",
      "100%|██████████| 20/20 [00:02<00:00,  6.94ba/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer to process the text and include a padding and truncation strategy\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Load Base Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Create Dataset\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/domy/Desktop/projects/nlp-helpful-product-review/env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6250\n",
      "  Number of trainable parameters = 66955010\n",
      "  0%|          | 0/6250 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  8%|▊         | 500/6250 [22:01<4:14:30,  2.66s/it]Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4516, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      " 16%|█▌        | 1000/6250 [45:49<2:44:36,  1.88s/it]Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4154, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      " 20%|██        | 1250/6250 [59:25<3:16:52,  2.36s/it]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "                                                     \n",
      " 20%|██        | 1250/6250 [1:00:34<3:16:52,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34471622109413147, 'eval_accuracy': 0.848, 'eval_runtime': 69.0928, 'eval_samples_per_second': 28.947, 'eval_steps_per_second': 1.809, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1500/6250 [1:09:34<2:40:15,  2.02s/it] Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3488, 'learning_rate': 1.5200000000000002e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      " 32%|███▏      | 2000/6250 [1:28:12<3:01:26,  2.56s/it]Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3103, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      " 40%|████      | 2500/6250 [1:47:59<2:56:20,  2.82s/it]Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3146, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "                                                       \n",
      " 40%|████      | 2500/6250 [1:49:22<2:56:20,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37368935346603394, 'eval_accuracy': 0.842, 'eval_runtime': 78.6375, 'eval_samples_per_second': 25.433, 'eval_steps_per_second': 1.59, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 3000/6250 [6:47:18<11:31:14, 12.76s/it]    Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2165, 'learning_rate': 1.04e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      " 56%|█████▌    | 3500/6250 [10:30:57<1:48:29,  2.37s/it]    Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2056, 'learning_rate': 8.8e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
      " 60%|██████    | 3750/6250 [10:42:06<1:43:07,  2.47s/it]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "                                                        \n",
      " 60%|██████    | 3750/6250 [10:43:16<1:43:07,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5002440214157104, 'eval_accuracy': 0.832, 'eval_runtime': 70.2614, 'eval_samples_per_second': 28.465, 'eval_steps_per_second': 1.779, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 4000/6250 [10:54:56<1:41:14,  2.70s/it] Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.162, 'learning_rate': 7.2000000000000005e-06, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      " 72%|███████▏  | 4500/6250 [11:33:28<1:18:52,  2.70s/it]Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1402, 'learning_rate': 5.600000000000001e-06, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
      " 80%|████████  | 5000/6250 [11:58:46<1:16:53,  3.69s/it]Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1371, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "                                                        \n",
      " 80%|████████  | 5000/6250 [12:00:26<1:16:53,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6700233817100525, 'eval_accuracy': 0.8355, 'eval_runtime': 96.1836, 'eval_samples_per_second': 20.794, 'eval_steps_per_second': 1.3, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 5500/6250 [12:23:45<43:38,  3.49s/it]   Saving model checkpoint to ./results/checkpoint-5500\n",
      "Configuration saved in ./results/checkpoint-5500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0887, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n",
      " 96%|█████████▌| 6000/6250 [12:50:49<11:48,  2.83s/it]  Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0912, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
      "100%|██████████| 6250/6250 [13:02:56<00:00,  2.88s/it]The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 16\n",
      "                                                      \n",
      "100%|██████████| 6250/6250 [13:05:24<00:00,  2.88s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 6250/6250 [13:05:25<00:00,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.80378657579422, 'eval_accuracy': 0.8295, 'eval_runtime': 148.5959, 'eval_samples_per_second': 13.459, 'eval_steps_per_second': 0.841, 'epoch': 5.0}\n",
      "{'train_runtime': 47124.9625, 'train_samples_per_second': 2.122, 'train_steps_per_second': 0.133, 'train_loss': 0.23455033325195312, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6250, training_loss=0.23455033325195312, metrics={'train_runtime': 47124.9625, 'train_samples_per_second': 2.122, 'train_steps_per_second': 0.133, 'train_loss': 0.23455033325195312, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# training the model \n",
    "\n",
    "# Metrics \n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Set up Training job\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file results/checkpoint-6000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"results/checkpoint-6000/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file results/checkpoint-6000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at results/checkpoint-6000/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9988353848457336}]\n",
      "[{'label': 'LABEL_1', 'score': 0.9727368354797363}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"results/checkpoint-6000/\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"results/checkpoint-6000/\"\n",
    ")\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "result = pipe(\"I absolutely loved this product.\")\n",
    "result2test = pipe(\"same as advertised.\")\n",
    "print(result)\n",
    "print(result2test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model using Huggingface evaluate model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/checkpoint-6000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"results/checkpoint-6000/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file results/checkpoint-6000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"results/checkpoint-6000/\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file results/checkpoint-6000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at results/checkpoint-6000/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 553kB/s]\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8295, 'total_time_in_seconds': 108.41167458400014, 'samples_per_second': 18.44819764729627, 'latency_in_seconds': 0.05420583729200007}\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    pipeline,\n",
    ")\n",
    "from evaluate import evaluator\n",
    "import evaluate\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-classification\", model=\"results/checkpoint-6000/\"\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv(\"helpful_sentences_test.csv\")\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# evaluator\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# evaluating accuracy\n",
    "eval = evaluator(\"text-classification\")\n",
    "result = eval.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=test_dataset,\n",
    "    metric=accuracy,\n",
    "    label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
    " # type: ignore    strategy=\"bootstrap\",\n",
    "    n_resamples=200,\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate F1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.77k/6.77k [00:00<00:00, 200kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': {'confidence_interval': (0.8631611716917896, 0.8895750583302473), 'standard_error': 0.006637136238868675, 'score': 0.8764940239043826}, 'total_time_in_seconds': 111.57373449599254, 'samples_per_second': 17.9253657595537, 'latency_in_seconds': 0.055786867247996266}\n"
     ]
    }
   ],
   "source": [
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "result = eval.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=test_dataset,\n",
    "    metric=f1_metric,\n",
    "    label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
    "    strategy=\"bootstrap\",\n",
    "    n_resamples=200,\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16e9eed24398b4315f8faa1b3190f7828ba76140a3e980b0d4175a9590a28384"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
